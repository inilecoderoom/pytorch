{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It basically decides whether the neuron should be activated or not <br>\n",
    "If yes then How it should be activated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear activation is not good as it is not suited for complex tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With non-linear transformation the network can learn better and perform more complex tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch as tr\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation Function\n",
    "# 1. step function --> not used in practice\n",
    "# 2. sigmoid function --> used in the last layer of the binary classification problem\n",
    "# 3. tanH function --> f(x) = (2/(1 + exp(-2x)))-1 --> used in hidden layer\n",
    "# 4. Relu function --> f(x) = max(0, x) --> used in hidden layer to add non-linearity in neural network\n",
    "# 5. Leaky ReLU function --> f(x) = {x if x>=0 else a*x} {a is very small number like 0.01} --> solves VANISHING GRADIENT problem \n",
    "# 6. softmax function --> f(x) = exp(y(i))/sum(exp(y(i))) --> used in multiclass classification problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet_ActivationFunction(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(NeuralNet_ActivationFunction, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_size, 1) # num_classes is always 1\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        # need sigmoid in last layer of neural network\n",
    "        out= self.sigmoid(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "model = NeuralNet_ActivationFunction(input_size = 28*28, hidden_size = 5)\n",
    "criterion  =nn.BCELoss() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another way to use activation function by calling mathods directly\n",
    "class NeuralNet_ActivationFunction(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(NeuralNet_ActivationFunction, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        # self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_size, 1) # num_classes is always 1\n",
    "        # self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # F.leaky_relu() # available in functional api\n",
    "        out = tr.relu(self.linear1(x))\n",
    "        out = tr.sigmoid(self.linear2(out))   \n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "model = NeuralNet_ActivationFunction(input_size = 28*28, hidden_size = 5)\n",
    "criterion  =nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('iNile')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "35c5b4e930fdcb47ac81caa69f444b3cabeb1c3585cb7f94cf2c3a5cee17fbe3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
