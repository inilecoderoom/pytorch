{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading important libraries\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import glob\n",
    "import torch.nn as nn\n",
    "\n",
    "import torchvision\n",
    "import pathlib\n",
    "\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# checking for device existence\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')   \n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms\n",
    "transformer = transforms.Compose([\n",
    "    transforms.Resize((150,150)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(), #from 0-255 to 0-1, numpy to tensors\n",
    "    transforms.Normalize([0.5,0.5,0.5], # from 0-1 to [-1,1], formula (x-mean)/std\n",
    "                        [0.5,0.5,0.5])\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TRAIN_DIRECTORY = \"../datasets/scenary_detection_data/scenary_train/\"\n",
    "TEST_DIRECTORY = \"../datasets/scenary_detection_data/scenary_test/\"\n",
    "PREDICTION_DATA = \"../datasets/scenary_detection_data/scenary_pred/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATALOADER\n",
    "train_loader = DataLoader(\n",
    "    torchvision.datasets.ImageFolder(TRAIN_DIRECTORY, transform = transformer),\n",
    "    batch_size = 256, shuffle = True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    torchvision.datasets.ImageFolder(TEST_DIRECTORY, transform = transformer),\n",
    "    batch_size = 256, shuffle = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['buildings', 'forest', 'glacier', 'mountain', 'sea', 'street']\n"
     ]
    }
   ],
   "source": [
    "# categories\n",
    "root = pathlib.Path(TRAIN_DIRECTORY)\n",
    "classes = sorted([i.name.split('/')[-1] for i in root.iterdir()])\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN network \n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes = 6):\n",
    "        super(ConvNet, self).__init__()\n",
    "\n",
    "        # Input shape = (256,3,150,150)\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=12, kernel_size=3, stride=1, padding=1)\n",
    "        # shape = (256,12,150,150)\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=12)\n",
    "        # shape = (256,12,150,150)\n",
    "        self.relu1=  nn.ReLU()\n",
    "        # shape = (256,12,150,150)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "        # shape = (256,12,75,75)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=12, out_channels=20, kernel_size=3, stride=1, padding=1)\n",
    "        # shape = (256,20,75,75)\n",
    "        self.relu2=  nn.ReLU()\n",
    "        # shape = (256,20,75,75)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=20, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        # shape = (256,32,75,75)\n",
    "        self.bn3 = nn.BatchNorm2d(num_features=32)\n",
    "        # shape = (256,32,75,75)\n",
    "        self.relu3=  nn.ReLU()\n",
    "        # shape = (256,32,75,75)\n",
    "\n",
    "        self.fc = nn.Linear(in_features=32*75*75, out_features=num_classes)\n",
    "\n",
    "    \n",
    "    # Feed forward function\n",
    "    def forward(self, input):\n",
    "        output = self.conv1(input)\n",
    "        output = self.bn1(output)\n",
    "        output = self.relu1(output)\n",
    "\n",
    "        output = self.pool(output)\n",
    "\n",
    "        output = self.conv2(output)\n",
    "        output = self.relu2(output)\n",
    "\n",
    "        output = self.conv3(output)\n",
    "        output = self.bn3(output)\n",
    "        output = self.relu3(output)\n",
    "\n",
    "        # this output is in matrix form with shape of (256,32,75,75)\n",
    "\n",
    "        output = output.view(-1,32*75*75)\n",
    "        output = self.fc(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNet(num_classes=6).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optimizer and loss function\n",
    "optimizer = Adam(model.parameters(),lr=0.001,weight_decay=0.0001)\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the size of training and testing images\n",
    "train_count = len(glob.glob(TRAIN_DIRECTORY+'/**/*.jpg'))\n",
    "test_count = len(glob.glob(TEST_DIRECTORY+'/**/*.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14034 3000\n"
     ]
    }
   ],
   "source": [
    "print(train_count,test_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Train_Loss: 18 Train Accuracy: 0.49401453612654983 Test Accuracy: 0.633\n",
      "Epoch: 1 Train_Loss: 1 Train Accuracy: 0.7245974062989882 Test Accuracy: 0.6563333333333333\n",
      "Epoch: 2 Train_Loss: 0 Train Accuracy: 0.7902949978623344 Test Accuracy: 0.7273333333333334\n",
      "Epoch: 3 Train_Loss: 0 Train Accuracy: 0.854353712412712 Test Accuracy: 0.739\n",
      "Epoch: 4 Train_Loss: 0 Train Accuracy: 0.8852786090922047 Test Accuracy: 0.7376666666666667\n"
     ]
    }
   ],
   "source": [
    "# Model training and saving the model\n",
    "best_accuracy = 0.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # Evalution and training on training dataset\n",
    "    model.train()\n",
    "    train_accuracy = 0.0\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            images = Variable(images.cuda())\n",
    "            labels = Variable(labels.cuda())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.cpu().data*images.size(0)\n",
    "        _,prediction = torch.max(outputs.data, 1)\n",
    "\n",
    "        train_accuracy += int(torch.sum(prediction == labels.data))\n",
    "\n",
    "    train_accuracy = train_accuracy/train_count\n",
    "    train_loss = train_loss/train_count\n",
    "\n",
    "    # Evalution on testing data\n",
    "    model.eval()\n",
    "\n",
    "    test_accuracy = 0.0\n",
    "    for i, (images, labels) in enumerate(test_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            images = Variable(images.cuda())\n",
    "            labels = Variable(labels.cuda())\n",
    "\n",
    "        outputs = model(images)\n",
    "        _,prediction = torch.max(outputs.data, 1)\n",
    "        test_accuracy += int(torch.sum(prediction == labels.data))\n",
    "\n",
    "    test_accuracy = test_accuracy/test_count\n",
    "\n",
    "    print('Epoch:' +str(epoch)+' -->'+' Train_Loss: ' +str(int(train_loss))+','+' Train Accuracy: ' +str(train_accuracy)+','+' Test Accuracy: ' +str(test_accuracy))\n",
    "\n",
    "    # Save the best model\n",
    "    if test_accuracy>best_accuracy:\n",
    "        torch.save(model.state_dict(), 'best_checkpoint.model')\n",
    "        best_accuracy = test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvNet(\n",
       "  (conv1): Conv2d(3, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu1): ReLU()\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(12, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (relu2): ReLU()\n",
       "  (conv3): Conv2d(20, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu3): ReLU()\n",
       "  (fc): Linear(in_features=180000, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_m = torch.load('best_checkpoint.model')\n",
    "model = ConvNet(num_classes=6)\n",
    "model.load_state_dict(checkpoint_m)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms\n",
    "transformer_pred = transforms.Compose([\n",
    "    transforms.Resize((150,150)),\n",
    "    transforms.ToTensor(), #from 0-255 to 0-1, numpy to tensors\n",
    "    transforms.Normalize([0.5,0.5,0.5], # from 0-1 to [-1,1], formula (x-mean)/std\n",
    "                        [0.5,0.5,0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction function\n",
    "def predict_model(img_path, transformer_pred):\n",
    "    image = Image.open(img_path)\n",
    "    image_tensor = transformer_pred(image).float()\n",
    "    image_tensor = image_tensor.unsqueeze_(0)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        image_tensor.cuda()\n",
    "\n",
    "    input = Variable(image_tensor)\n",
    "\n",
    "    output = model(input)\n",
    "    index = output.data.numpy().argmax()\n",
    "    pred = classes[index]\n",
    "    return pred \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_path = glob.glob(PREDICTION_DATA+'/*.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dict = {}\n",
    "\n",
    "for i in images_path: \n",
    "    pred_dict[i[i.rfind('/')+1:]] = predict_model(i, transformer_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7301"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pred_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'4149.jpg': 'sea', '12550.jpg': 'buildings', '4986.jpg': 'glacier', '2595.jpg': 'glacier', '6682.jpg': 'street', '13550.jpg': 'sea', '4668.jpg': 'sea', '23856.jpg': 'street', '4166.jpg': 'glacier', '9433.jpg': 'forest'}\n"
     ]
    }
   ],
   "source": [
    "print(dict(list(pred_dict.items())[5:15]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('iNile')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "35c5b4e930fdcb47ac81caa69f444b3cabeb1c3585cb7f94cf2c3a5cee17fbe3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
